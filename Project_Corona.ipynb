{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the spread of COVID-19 based on European traffic data during summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Data\n",
    "\n",
    "- Our World in Data COVID-19 dataset\n",
    "    - Data on deaths in different countries.\n",
    "\n",
    "\n",
    "- Google's traffic data \n",
    "    - Google provides anonymized insights from products such as Google Maps for researchers to help them to make critical analysis to combat COVID-19. \n",
    "    - Google has divided their traffic data into six traffic components: \n",
    "        1. retail \\& recreation\n",
    "            - places like restaurants, cafes, shopping centers, theme parks, museums, libraries and movie theaters\n",
    "        2. grocery \\& pharmacy\n",
    "            - places like grocery markets, food warehouses, farmers markets, specialty food shops, drug stores and pharmacies\n",
    "        3. parks\n",
    "            - places like national parks, public beaches, marinas, dog parks, plazas and public gardens\n",
    "        4. transit stations\n",
    "            - places like public transport hubs such as subway, bus and train stations\n",
    "        5. workplaces \n",
    "            - places of work\n",
    "        6. residential \n",
    "            -  places of residence\n",
    "     \n",
    "  - These components do not tell anything how much time people spend in each section on average but they still give a lot of information how people's traffic behavior changed during the pandemic\n",
    " \n",
    " - The traffic data's baseline is counted as a median value of multiple days. Day-to-day changes should not be emphasized too much because they are effected on many different factors, f.e. the weather and public events.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2. Moral behind the Bayesian model\n",
    "\n",
    "\n",
    "- Getting the traffic data down slows down the spread of the virus. \n",
    "    - Several articles (Ferretti et al 2020, ECDC report, LSHTM report) have pointed out pre-symptomatic and asymptomatic  infections  play  a  significant  role  in  the  spread  of  COVID-19.  Indeed,  this observation  is  an  argument  it  may  not  be  enough  to  get  the  symptomatic  cases  to  stay at  home. Also  governmental  restrictions  should  be  implemented  to  get peopleâ€™s movement down and furthermore the pandemic under control.\n",
    "    - Essentially, the reason to implement non-pharmaceutical interventions is to get people's traffic data down!\n",
    "\n",
    "\n",
    "- There are multiple reasons why it makes sense to analyse European countries in this research\n",
    "    - COVID-19 hit European countries badly during autumn\n",
    "    - European governments have similar capabilities to restrict their citizens movement in comparison to many countries, f.e. China\n",
    "    - European countries adapted suppression strategy instead of mitigation one\n",
    "    \n",
    "    \n",
    "- There are many major differences between European countries which effect on the spread of COVID-19\n",
    "    - Examples: different age distributions, different population densities in cities, cultural differences\n",
    "    - Therefore comparisons between European countries should be avoided\n",
    "\n",
    "    \n",
    "- The COVID-19-case data is not reliable at least as the only measure about the development of the epidemic. COVID-19-death data has many benefits compared to the COVID-19 case data! \n",
    "    - The amount of testing varies a lot between countries\n",
    "    - Also using death data over infected data has the benefit that deaths measures much better country's success against the epidemic than infections\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.3. Motivation, research goals\n",
    "\n",
    "\n",
    "1. To create a model which predicts the spread of the epidemic well based on the traffic data and data on deaths.\n",
    "\n",
    "2. Based on the created model, trying to understand which of these Google's traffic components predicted the spread of COVID-19 in different European countries\n",
    "\n",
    "\n",
    "### 1.4.  Structure\n",
    "\n",
    "\n",
    "\n",
    "1. Introduction\n",
    "    - Describes the essentials of this notebook\n",
    "2. Getting an overview how COVID-19-cases and people's traffic behavior developed during the pandemic\n",
    "    - This section gives moral for sections 4-8.\n",
    "3. Dividing regions inside European countries in groups based on the development of the epidemic\n",
    "    - COVID-19 hit European countries very differently. Therefore they are divided in three different groups.\n",
    "4. Based on countries which did well against the epidemic during summer, understanding the impact of different traffic components to the spread of the epidemic\n",
    "    - This section uses the assumption: The spread of COVID-19 can be predicted with people's traffic behavior.\n",
    "    - This section gives understanding which Google traffic components had the biggest impact on the spread of COVID-19.\n",
    "    - Important section, contains results!\n",
    "5. Based on countries where the epidemic escalated during the summer, understanding the impact of different traffic components to the spread of the epidemic\n",
    "    - This section has the same aim as the section 5 has. However, the methods used in these sections strongly differ from each other \n",
    "    - Important section, contains results!\n",
    "6. Getting an overview if the traffic components with the most impact predicted well the spread of COVID-19 in different European countries\n",
    "    - NOT YET IMPLEMENTED AT ALL\n",
    "7. Summary\n",
    "    - NOT YET IMPLEMENTED AT ALL\n",
    "    - Summaries the whole notebook and also opens discussion about the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import linear_model \n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Parameters which need to be defined manually\n",
    "\n",
    "- All the parameters, which need to be defined manually, are here\n",
    "\n",
    "\n",
    "- During text there are detailed explanations what these parameters are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "\n",
    "# There is COVID-19-data from February until now\n",
    "observations_start_date = datetime.datetime(2020, 2, 1, 0, 0)\n",
    "observations_end_date = datetime.datetime(2020, 11, 15, 0, 0)\n",
    "\n",
    "# However, the data analysis of this notebook concentrates on autumn months, i.e. on so called tail\n",
    "tail_start_date = datetime.datetime(2020, 9, 1, 0, 0)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# European countries ordered by population\n",
    "european_countries = [\n",
    "    'Germany', 'United Kingdom', 'France', 'Italy', 'Spain', \n",
    "    'Ukraine', 'Poland', 'Romania','Netherlands', 'Belgium', \n",
    "    'Greece', 'Sweden', 'Portugal', 'Hungary', 'Belarus', \n",
    "    'Austria', 'Switzerland', 'Bulgaria', 'Serbia', 'Denmark', \n",
    "    'Finland', 'Norway', 'Ireland', 'Croatia', 'Moldova', \n",
    "    'Bosnia and Herzegovina', 'Lithuania', 'Slovenia', 'Estonia' ]\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "# Window size for convolution\n",
    "w = 7\n",
    "\n",
    "# Death limit\n",
    "d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 European countries analysed in this notebook.\n",
      "The length of the whole interval: 288\n",
      "The length of the tail interval: 75\n"
     ]
    }
   ],
   "source": [
    "# Follows directly from manual definitions \n",
    "num_countries = len(european_countries)\n",
    "whole_interval_len = (observations_end_date - observations_start_date).days \n",
    "tail_interval_len = (observations_end_date - tail_start_date).days \n",
    "date_list = [observations_start_date + datetime.timedelta(days=x) for x in range(whole_interval_len)]\n",
    "\n",
    "print(\"There are \" + str(num_countries) + \" European countries analysed in this notebook.\")\n",
    "print(\"The length of the whole interval: \" + str(whole_interval_len))\n",
    "print(\"The length of the tail interval: \" + str(tail_interval_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition: Autumn interval $\\tau_{\\text{summer}}$\n",
    "\n",
    "Let's denote the summer interval with $\\tau_{\\text{summer}} = \\{ 0,1,2, \\dots, 75 \\}$. Indeed $t = 0$ corresponds the date 1.9.2020, $t = 1$ corresponds the date 2.9.2020 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Adding previously cleaned data to dataframes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_countries\n",
    "\n",
    "- The length of this dataframe is 'num_countries'. Indeed, for each country there is one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Preprocessed_data/df_countries.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd92822688f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m           ])\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf_countries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Preprocessed_data/df_countries.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes_countries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Change date-columns from string-type to datetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Preprocessed_data/df_countries.csv'"
     ]
    }
   ],
   "source": [
    "# A dataframe sorted by countries\n",
    "\n",
    "dtypes_countries = np.dtype([\n",
    "          ('country', str),\n",
    "          ('group', float),\n",
    "          ('population', int),\n",
    "          ('population_in_millions', int),\n",
    "          ('significance_level', float),\n",
    "          ('control_point', str),\n",
    "          ('escalation_point_deaths', str),\n",
    "          ('deaths_escalated_rapidly', str),\n",
    "          ('escalation_point_infections', str),\n",
    "          ('infections_escalated_rapidly', str),\n",
    "          ])\n",
    "\n",
    "df_countries = pd.DataFrame(pd.read_csv('Data/Preprocessed_data/df_countries.csv', dtype=dtypes_countries))\n",
    "\n",
    "# Change date-columns from string-type to datetype\n",
    "df_countries['control_point'] = pd.to_datetime(df_countries['control_point'], format='%Y-%m-%d')\n",
    "df_countries['escalation_point_deaths'] = pd.to_datetime(df_countries['escalation_point_deaths'], format='%Y-%m-%d')\n",
    "df_countries['escalation_point_infections'] = pd.to_datetime(df_countries['escalation_point_infections'], format='%Y-%m-%d')\n",
    "\n",
    "# Show the dataframe\n",
    "df_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_days_by_countries\n",
    "\n",
    "- All the days to which there exists traffic and infected data to each country\n",
    "\n",
    "\n",
    "- The length of the dataframe equals 'num_countries' * 'whole_interval_len'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_days_by_countries = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('date', str), # current date. This will become datetime-time using parse_dates!\n",
    "          ('new_infections', int), # new infections on that date\n",
    "          ('new_infections_smooth', int), # smoothened new infections on that date\n",
    "          ('new_deaths', int), # new deaths on that date\n",
    "          ('new_deaths_smooth', int), # smoothened new deaths on that date\n",
    "          ('total_deaths_per_million', float), # how many deaths per million has occured until that date\n",
    "          ('traffic_retail', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces', float), # workplace traffic on that date\n",
    "          ('traffic_residential', float), # residential traffic on that date\n",
    "          ])\n",
    "\n",
    "df_days_by_countries = pd.DataFrame(pd.read_csv('Data/Preprocessed_data/df_days_by_countries.csv', dtype=dtypes_days_by_countries))   \n",
    "\n",
    "# Change the date-column from string-type to datetype\n",
    "df_days_by_countries['date'] = pd.to_datetime(df_days_by_countries['date'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "# Show the dataframe\n",
    "df_days_by_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_regions\n",
    "\n",
    "\n",
    "- Each country consists of smaller regions.\n",
    "\n",
    "\n",
    "- For each region there is one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_regions = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),   # region name\n",
    "          ('group', float),    # the group of the region. Remark: This should be str!\n",
    "          ])\n",
    "\n",
    "df_regions = pd.DataFrame(pd.read_csv('Data/Preprocessed_data/df_regions.csv', dtype=dtypes_regions)) \n",
    "\n",
    "# Show the dataframe\n",
    "df_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_days_by_regions\n",
    "\n",
    "\n",
    "- Similar to the dataframe 'df_days_by_countries' but instead of a country, each row equals a specific day of a region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A countrywise sorted dataframe s.t. for each day on the time interval of each country there is a row \n",
    "\n",
    "dtypes_days_by_regions = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),  # country name\n",
    "          ('date', str), # current date\n",
    "          ('new_infections', float), # new infections. These information is found out only for some regions!\n",
    "          ('traffic_retail', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces', float), # workplace traffic on that date\n",
    "          ('traffic_residential', float), # residential traffic on that date\n",
    "          ('traffic_retail_smooth', float), # retail and recreation traffic on that date\n",
    "          ('traffic_supermarket_smooth', float), # supermarket and pharmacy traffic on that date\n",
    "          ('traffic_parks_smooth', float),  # park traffic on that date\n",
    "          ('traffic_transit_stations_smooth', float), # transit station traffic on that date\n",
    "          ('traffic_workplaces_smooth', float), # workplace traffic on that date\n",
    "          ('traffic_residential_smooth', float), # residential traffic on that datep\n",
    "          ])\n",
    "\n",
    "df_days_by_regions = pd.DataFrame(pd.read_csv('Data/Preprocessed_data/df_days_by_regions.csv', dtype=dtypes_days_by_regions)) \n",
    "\n",
    "# Change the date-column from string-type to datetype\n",
    "df_days_by_regions['date'] = pd.to_datetime(df_days_by_regions['date'], format='%Y-%m-%d')\n",
    "\n",
    "# Show the dataframe\n",
    "df_days_by_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_group1_regions_and_traffic\n",
    "\n",
    "\n",
    "- Later in the notebook there is a more detailed explanation for group 1 -countries. Practically these are countries where the epidemic was under control during the whole summer.\n",
    "\n",
    "\n",
    "- For each traffic component and for each of these groups' region there is a row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_group1_regions_and_traffic = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),   # region name\n",
    "          ('traffic_component', str),   # current traffic component\n",
    "          ('highest_traffic_average', float), # the lowest traffic average of a 2 week period\n",
    "          ])\n",
    "data_group1_regions_and_traffic = np.empty(0, dtype=dtypes_group1_regions_and_traffic)\n",
    "df_group1_regions_and_traffic = pd.DataFrame(data_group1_regions_and_traffic)\n",
    "\n",
    "# Show the dataframe\n",
    "df_group1_regions_and_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe df_group2_regions_and_traffic\n",
    "\n",
    "\n",
    "- Similar to the dataframe 'df_group1_regions_and_traffic' but instead of group 1 -regions, this dataframe deals with group 2 -countries' regions. Group 2 -countries are countries where the epidemic was under control at first but later escalated again during summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_group2_regions_and_traffic = np.dtype([\n",
    "          ('country', str),  # country name\n",
    "          ('region', str),   # region name\n",
    "          ('traffic_component', str),   # current traffic component\n",
    "          ('traffic_average_before_escalation', float), # what was the traffic components value when the epidemic started to escalate \n",
    "          ('infected_coefficient', float), # With linear regression there will be fitted infected coefficient \n",
    "          ('infected_intercept', float),  # and intercept to the escalating infection data\n",
    "          ('traffic_coefficient', float),  # The same is done for the current traffic component\n",
    "          ('traffic_intercept', float),\n",
    "          ])\n",
    "data_group2_regions_and_traffic = np.empty(0, dtype=dtypes_group2_regions_and_traffic)\n",
    "df_group2_regions_and_traffic = pd.DataFrame(data_group2_regions_and_traffic)\n",
    "\n",
    "# Show the dataframe\n",
    "df_group2_regions_and_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting an overview how COVID-19-cases, -deaths and people's traffic behavior developed during the pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Plot all Google's traffic components and also death and infected data countrywise\n",
    "\n",
    "- The vertical black line represents when the tail of the pandemic starts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_days_by_countries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0f4ede76ade1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Define the current country, a temporary dataframe of the country and x-axis (dates)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcurrent_country\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuropean_countries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdf_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_days_by_countries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_days_by_countries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcurrent_country\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_current\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_days_by_countries' is not defined"
     ]
    }
   ],
   "source": [
    "# Define categories which are plotted\n",
    "traffic_components = ['traffic_retail', 'traffic_supermarket', 'traffic_parks', \n",
    "                      'traffic_transit_stations', 'traffic_workplaces', 'traffic_residential'\n",
    "                     ]\n",
    "\n",
    "description = ['traffic in retail and recreation', 'traffic in supermarkets and pharmacy', 'traffic in parks',\n",
    "               'traffic in transit stations', 'traffic in workplaces', 'traffic in residential ares'\n",
    "              ]\n",
    "\n",
    "\n",
    "# Loop over each country\n",
    "for i in range(num_countries):\n",
    "    \n",
    "    # Define the current country, a temporary dataframe of the country and x-axis (dates)\n",
    "    current_country = european_countries[i]\n",
    "    df_current = df_days_by_countries[(df_days_by_countries['country'] == current_country)]\n",
    "    x = df_current['date'].tolist() \n",
    "    \n",
    "    print('\\033[1m' + current_country)\n",
    "    \n",
    "    # Loop over each traffic component\n",
    "    for j in range(len(traffic_components)):\n",
    "\n",
    "        # Define y-components which are going to be plotted in one figure\n",
    "        y_traffic = df_current[traffic_components[j]].tolist()\n",
    "        y_infected = df_current['new_infections'].tolist()\n",
    "        y_deaths = df_current['new_deaths'].tolist()\n",
    "\n",
    "        # Define the figure and different y-axis (there are 3 in total: traffic, infected, deaths)\n",
    "        fig, host = plt.subplots(figsize=(26, 6))\n",
    "        fig.subplots_adjust(right=0.75)\n",
    "        par1 = host.twinx()\n",
    "        par2 = host.twinx()\n",
    "\n",
    "        # Set the most right one y-axis to right\n",
    "        par2.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "\n",
    "        # Plot the traffic, infected and death data\n",
    "        p1, = host.plot(x, y_traffic, \"b-\", label='The change in ' + description[j] +  ' (%)' )\n",
    "        p2, = par1.plot(x, y_infected, marker = 'o', linestyle='', color = \"red\", label=\"New infections ()\") \n",
    "        p3, = par2.plot(x, y_deaths, marker = 'o', linestyle='', color = \"black\", label=\"New deaths ()\")\n",
    "\n",
    "        # Define the texts\n",
    "        host.set_ylabel('The change in ' + description[j] +  ' (%)')\n",
    "        par1.set_ylabel(\"New infections ()\")\n",
    "        par2.set_ylabel(\"New deaths ()\")\n",
    "\n",
    "        # Text on the axis with the correct color\n",
    "        host.yaxis.label.set_color(p1.get_color())\n",
    "        par1.yaxis.label.set_color(p2.get_color())\n",
    "        par2.yaxis.label.set_color(p3.get_color())\n",
    "\n",
    "        # Make little spikes for different y-axis\n",
    "        tkw = dict(size=30, width=1.6)\n",
    "        host.tick_params(axis='y', colors=p1.get_color(), **tkw)\n",
    "        par1.tick_params(axis='y', colors=p2.get_color(), **tkw)\n",
    "        par2.tick_params(axis='y', colors=p3.get_color(), **tkw)\n",
    "        host.tick_params(axis='x', **tkw)\n",
    "\n",
    "        plt.axvline(tail_start_date, color='black')\n",
    "        \n",
    "        print('\\033[0m' + description[j])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Conclusions\n",
    "\n",
    "- Based on previous plots, clearly residential traffic does not impact negatively on the spread of COVID-19. The effect of other traffic components will be analysed more detailed and residential traffic data is not analysed any more in this project.\n",
    "    \n",
    "    \n",
    "- Workplace data varies a lot. Therefore the smoothened workplace data ignores the values during weekends!\n",
    "\n",
    "\n",
    "- It is difficult to analyse the impact of different traffic components to the spread of COVID-19 based on the first local maximum of the epidemic (in most countries it happend on March or on April)\n",
    "    \n",
    "    - Almost in every European country all the traffic data components except residential traffic went strongly down at the same time. Therefore, it is difficult to say which traffic component truely mattered based on the beginning of the pandemic.\n",
    "    \n",
    "    \n",
    "- Therefore, let's concentrate the analysis what happend later after the first local maximum, i.e. concentrate on the tail of the pandemic which is defined to start on 1.6.2020\n",
    "    - Concentrating on the tail has the benefit that people have been aware of COVID-19 since then. In many countries COVID-19 started to spread fast because people did not take the pandemic seriously in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dividing regions inside European countries in groups based on the development of the epidemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Moral\n",
    "\n",
    "\n",
    "- Based on the previous conclusions, it is reasonable to concentrate the analysis on the tail of the pandemic. The tail interval practically means all the days in June-August.\n",
    "\n",
    "\n",
    "- European countries have major differences how well they were capable of keeping the epidemic under control. Some countries were very succesful but some had difficulties. Therefore, European countries are divided in three groups especially based on their smoothened death data $\\bar{D}_{t, c}$ but also smoothened infected data $\\bar{C}_{t, c}$ based on the tail interval\n",
    "    - Group 1: countries where the epidemic was under control during the whole summer\n",
    "    - Group 2: countries where the epidemic suddenly escalated during summer \n",
    "    - Group 3: countries with too much noise\n",
    "        - Indeed, countries in group 3 will not be analysed later! \n",
    "\n",
    "\n",
    "- Regions inside European countries provide much more fine-grained data than the countries themselves. Indeed, the success against the epidemic may have varied strongly inside a country! \n",
    "    - It is natural to assume all the regions inside group 1 -countries had the epidemic under control\n",
    "        - If the epidemic escalated in one region inside the country, this information would have appeared in countrywise infected and death data as well!\n",
    "    - However, inside group 2 -countries there are regions where the epidemic escalated and there are regions where it did not!\n",
    "\n",
    "\n",
    "- Analysing European regions this way has two main benefits\n",
    "    1. Both infected and death data are now taken into account but the emphasis is on the death data.\n",
    "    2. Regional data is not easily available. This approach is a shortcut to find out the regions to which infected data is searched manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. An exact description how countries are divided in groups\n",
    "\n",
    "\n",
    "- European countries are divided in groups based on three different statements and the definition of significance level.\n",
    "\n",
    "\n",
    "##### Definition: Significance level\n",
    "\n",
    "- A country $c$'s significance level: $\\alpha_c = \\text{max}(c_1, c_2 \\cdot p_c)$ $\\quad  || \\: c_1 = 5, c_2 = 1$\n",
    "    - A country $i$'s population in millions $p_c \\in \\mathbb{N}$ is a constant as well\n",
    "    - Intuitively, if a country's smoothened deaths are under its significant level, then there are very few deaths in the country\n",
    "\n",
    "\n",
    "##### Statement 1: The epidemic was under control at some point during summer \n",
    "\n",
    "\n",
    "- A country $c$ had the epidemic under control at some point during summer if there is a day $t$ to which     \n",
    "    - $\\exists t' \\geq t$ s.t. \n",
    "        - $\\bar{D}_{t', c} < \\text{max}(\\alpha_c, c_3 \\cdot \\bar{D}_{t, c})$  $\\quad  \\text{|| Smoothened deaths must be halved or below the significance level,  } c_3= \\frac{1}{2}$\n",
    "        - AND $\\bar{D}_{\\text{max}(t,t'' - w), c} \\geq \\bar{D}_{t'', c}, \\forall t'' \\in \\{ t, t+1, \\dots, t'\\}$ $\\quad  \\text{|| There must be constantly an increasing trend}$\n",
    "\n",
    "\n",
    "- If a country had the epidemic under control, let's denote the first day when the epidemic was under control with $t_{\\text{con}}$.\n",
    "\n",
    "\n",
    "##### Statement 2: After the epidemic was under control, the smoothened deaths increased\n",
    "\n",
    "\n",
    "\n",
    "- Firstly, the country must fulfil the statement 1 so that the statement 2 can be true.\n",
    "\n",
    "\n",
    "- Secondly, the smoothened deaths escalates fast enough in the country on a day $t > t_c$ if\n",
    "    - $\\exists t' > t$ s.t. \n",
    "        - $\\bar{D}_{t', c} > \\text{max}(\\alpha_c, c_4 \\cdot \\bar{D}_{t, c})$   $\\quad  \\text{||Â Smoothened deaths must be doubled and above the significance level,  } c_4= 2$\n",
    "        - AND $\\bar{D}_{\\text{max}(t,t'' - w), c} \\leq \\bar{D}_{t'', c}, \\forall t'' \\in \\{ t, t+1, \\dots, t'\\}$ $\\quad  \\text{|| There must be constantly an increasing trend}$\n",
    "        - AND $\\bar{D}_{t'', c} \\geq c_5 , \\forall t'' \\in \\{ t, t+1, \\dots, t'\\}$ $\\quad  \\text{|| If there isn't enough deaths, data is too noisy to analyse trends, } c_5= 5$\n",
    "\n",
    "\n",
    "- Let's denote the first day when the smoothened deaths started to escalate with $t_{D, \\text{esc}}$.\n",
    "\n",
    "\n",
    "##### Additional condition to the statement 2: After the epidemic was under control, the smoothened deaths did not only increase but they increased fast enough\n",
    "\n",
    "\n",
    "- Furthermore, if $(t' - t) \\leq c_6 = 14$, the epidemic escalates rapidly in a country $c$.\n",
    "\n",
    "\n",
    "\n",
    "#### Dividing countries and regions in groups\n",
    "\n",
    "- A country is in \n",
    "    - Group 1, if it fullfills the statement 1 but not the statement 2\n",
    "    - Group 2, if it fullfills statements 1 and 2 and the additional condition to the statement 2\n",
    "    - Group 3, otherwise\n",
    "    \n",
    "\n",
    "- Furthermore, a region is in \n",
    "    - Group 1, if it belongs to a Group 1 -country\n",
    "    - Group 2, if it belongs to a Group 2 -country\n",
    "    - Group 3, if it belongs to a Group 3 -country\n",
    "    \n",
    "    \n",
    "#### For group 2 -countries, there will be manually found out the date when the epidemic started to change based on their infected data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. A practical implementation for dividing the countries in groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_1 = 5\n",
    "c_2 = 1\n",
    "c_3 = 0.5\n",
    "c_4 = 2\n",
    "c_5 = 5\n",
    "c_6 = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the significance level for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each country\n",
    "for c in european_countries:\n",
    "    \n",
    "    # Get the p_c from the dataframe\n",
    "    current_population_in_millions = df_countries.loc[df_countries['country'] == c, 'population_in_millions'].tolist()[0]\n",
    "    \n",
    "    # Add the significance level to the dataframe\n",
    "    df_countries.loc[df_countries['country'] == c, 'significance_level'] = max(c_1, c_2 * current_population_in_millions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the statement 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_with_control_point = []\n",
    "\n",
    "# Loop over each country \n",
    "for c in european_countries: \n",
    "\n",
    "    # The significance level alpha_c \n",
    "    alpha_c = df_countries.loc[df_countries['country'] == c, 'significance_level'].tolist()[0] \n",
    "\n",
    "    # Smoothened deaths of the summer interval of the country c\n",
    "    D_c = df_days_by_countries[(df_days_by_countries['country'] == c)\n",
    "                & (df_days_by_countries['date'] >= tail_start_date)\n",
    "                & (df_days_by_countries['date'] <= observations_end_date)]['new_deaths_smooth'].tolist()\n",
    "\n",
    "    # The whole tail will be scanned and found out if there exists a control day at some point \n",
    "    t_con_found = False \n",
    "    t_con = 0 \n",
    "    t = 0 \n",
    "\n",
    "\n",
    "    # Loop over each day in the tail interval as long as t_con is not found\n",
    "    while (t < tail_interval_len) and (t_con_found == False):\n",
    "\n",
    "        # Let's loop as long as t can be the control day t_con\n",
    "        t_possibly_t_con = True\n",
    "\n",
    "        # The value for smoothened deaths\n",
    "        D_t_c = D_c[t]\n",
    "\n",
    "        # Set t' to be t at first and then start increasing it one by one\n",
    "        t_prime = t\n",
    "\n",
    "\n",
    "        # Loop as long as t_prime did not exceed the summer interval and t can be possibly t_con and\n",
    "        # a suitable t_con is not yet found\n",
    "        while (t_prime < tail_interval_len) and t_possibly_t_con and (t_con_found == False):\n",
    "\n",
    "            # The value for smoothened deaths\n",
    "            D_t_prime_c = D_c[t_prime]\n",
    "\n",
    "            # The compared value \n",
    "            D_value_compared = D_c[max(t, t_prime - w)]\n",
    "\n",
    "            if D_value_compared < D_t_prime_c:\n",
    "                t_possibly_t_con = False\n",
    "\n",
    "            if t_possibly_t_con and (D_t_prime_c < max(alpha_c, c_3 * D_t_c)):\n",
    "\n",
    "                t_con_found = True\n",
    "                t_con = t\n",
    "\n",
    "                # Add value to the dataframe\n",
    "                df_countries.loc[df_countries['country'] == c, 'control_point'] = tail_start_date + datetime.timedelta(t)\n",
    "\n",
    "                # Add the current country to countries which have a control point\n",
    "                countries_with_control_point.append(c)\n",
    "                \n",
    "            t_prime += 1\n",
    "\n",
    "        t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the statement 2 and the additional condition for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_epidemic_escalated = []\n",
    "countries_deaths_escalated_rapidly = []\n",
    "\n",
    "# Loop over each country \n",
    "for c in countries_with_control_point: \n",
    "\n",
    "    # The significance level alpha_c \n",
    "    alpha_c = df_countries.loc[df_countries['country'] == c, 'significance_level'].tolist()[0] \n",
    "\n",
    "    # The control point of a country \n",
    "    control_point = df_countries.loc[(df_countries['country'] == c), 'control_point'].tolist()[0] \n",
    "\n",
    "    # Smoothened deaths of the summer interval of the country c\n",
    "    D_c = df_days_by_countries[(df_days_by_countries['country'] == c)\n",
    "                & (df_days_by_countries['date'] >= tail_start_date)\n",
    "                & (df_days_by_countries['date'] <= observations_end_date)]['new_deaths_smooth'].tolist()\n",
    "\n",
    "    # The whole tail will be scanned and found out if there exists an escalation day at some point \n",
    "    t_esc_found = False \n",
    "    t_esc = 0 # Initialize just some value\n",
    "\n",
    "    t = (control_point - tail_start_date).days + 1\n",
    "\n",
    "\n",
    "    # Loop over each day in the tail interval as long as t_con is not found\n",
    "    while (t < tail_interval_len) and (t_esc_found == False):\n",
    "\n",
    "        # Let's loop as long as t can be the control day t_con\n",
    "        t_possibly_t_esc = True\n",
    "\n",
    "        # The value for smoothened deaths \n",
    "        D_t_c = D_c[t]\n",
    "\n",
    "        # Set t' to be t at first and then start increasing it one by one\n",
    "        t_prime = t\n",
    "\n",
    "\n",
    "        # Loop as long as t_prime did not exceed the summer interval and t can be possibly t_con and \n",
    "        # a suitable t_con is not yet found\n",
    "        while (t_prime < tail_interval_len) and t_possibly_t_esc and (t_esc_found == False): \n",
    "\n",
    "            # The value for smoothened deaths \n",
    "            D_t_prime_c = D_c[t_prime]\n",
    "\n",
    "            # The compared value \n",
    "            D_value_compared = D_c[max(t, t_prime - w)]\n",
    "\n",
    "            if D_value_compared > D_t_prime_c or D_t_prime_c < c_5:\n",
    "                t_possibly_t_esc = False\n",
    "\n",
    "\n",
    "            if t_possibly_t_con and (D_t_prime_c > max(alpha_c, c_4 * D_t_c)):\n",
    "            #if t_possibly_t_esc and (D_t_prime_c > max(10, 2 * D_t_c)):\n",
    "                t_esc_found = True\n",
    "                t_esc = t\n",
    "\n",
    "                # Add value to the dataframe\n",
    "                df_countries.loc[df_countries['country'] == c, 'escalation_point_deaths'] = tail_start_date + datetime.timedelta(t)\n",
    "\n",
    "                countries_epidemic_escalated.append(c)\n",
    "                \n",
    "                # Check if the epidemic escalated fast enough\n",
    "                if t_prime - t < c_6:\n",
    "                    df_countries.loc[df_countries['country'] == c, 'deaths_escalated_rapidly'] = \"yes\"\n",
    "                    countries_deaths_escalated_rapidly.append(c)\n",
    "                else:\n",
    "                    df_countries.loc[df_countries['country'] == c, 'deaths_escalated_rapidly'] = \"no\"\n",
    "\n",
    "            t_prime += 1\n",
    "\n",
    "        t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the statements, conclude to which group each country and furthermore each region belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add group 1 countries to the dataframe\n",
    "group_1_countries = list(set(countries_with_control_point) - set(countries_epidemic_escalated))\n",
    "df_countries.loc[df_countries['country'].isin(group_1_countries), ['group']] = 1\n",
    "\n",
    "# Add group 2 countries to the dataframe\n",
    "group_2_countries = countries_deaths_escalated_rapidly\n",
    "df_countries.loc[df_countries['country'].isin(group_2_countries), ['group']] = 2\n",
    "\n",
    "# Add group 3 countries to the dataframe\n",
    "group_3_countries = list(set(european_countries) - set(group_1_countries) - set(group_2_countries))\n",
    "df_countries.loc[df_countries['country'].isin(group_3_countries), ['group']] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For group 2 -countries the date when infections started to escalate are searched manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.loc[(df_countries['country'] == 'Serbia'), 'escalation_point_infections'] = datetime.datetime(2020, 6, 17, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A TEMPORARY CELL which will change Netherlands and Spain in group 2\n",
    "\n",
    "- Netherlands is not yet in group 2 because its death data is not out yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.loc[(df_countries['country'] == 'Netherlands'), 'group'] = 2\n",
    "\n",
    "df_countries.loc[(df_countries['country'] == 'Netherlands'), 'escalation_point_deaths'] = observations_end_date\n",
    "df_countries.loc[(df_countries['country'] == 'Netherlands'), 'deaths_escalated_rapidly'] = \"yes\"\n",
    "df_countries.loc[(df_countries['country'] == 'Netherlands'), 'escalation_point_infections'] = datetime.datetime(2020, 7, 16, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.loc[(df_countries['country'] == 'Spain'), 'group'] = 2\n",
    "\n",
    "df_countries.loc[(df_countries['country'] == 'Spain'), 'escalation_point_deaths'] = observations_end_date\n",
    "df_countries.loc[(df_countries['country'] == 'Spain'), 'deaths_escalated_rapidly'] = \"yes\"\n",
    "df_countries.loc[(df_countries['country'] == 'Spain'), 'escalation_point_infections'] = datetime.datetime(2020, 7, 16, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot each country's infections and deaths and possibly its control and escalation days with explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop over each country\n",
    "for i in range(num_countries):\n",
    "    \n",
    "    # Define the current country, a temporary dataframe of the country and x-axis (dates)\n",
    "    current_country = european_countries[i]\n",
    "    current_group = df_countries.loc[(df_countries['country'] == current_country), 'group'].tolist()[0]\n",
    "    print('\\033[1m' + current_country + ': GROUP ' + str(current_group))\n",
    "    \n",
    "    df_current = df_days_by_countries[(df_days_by_countries['country'] == current_country)\n",
    "                                    & (df_days_by_countries['date'] >= tail_start_date)]\n",
    "    x = df_current['date'].tolist() \n",
    "    \n",
    "    # Define y-components which are going to be plotted in one figure\n",
    "    y_infected = df_current['new_infections'].tolist()\n",
    "    y_deaths = df_current['new_deaths'].tolist()\n",
    "\n",
    "    # Define the figure and different y-axis (there are 3 in total: traffic, infected, deaths)\n",
    "    fig, host = plt.subplots(figsize=(26, 6))\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "    par1 = host.twinx()\n",
    "\n",
    "    # Plot the traffic, infected and death data\n",
    "    p1, = host.plot(x, y_infected, marker = 'o', linestyle='', color = \"red\", label=\"New infections ()\" )\n",
    "    p2, = par1.plot(x, y_deaths, marker = 'o', linestyle='', color = \"black\", label=\"New deaths ()\") \n",
    "\n",
    "    # Define the texts\n",
    "    host.set_ylabel(\"New infections ()\")\n",
    "    par1.set_ylabel(\"New deaths ()\")\n",
    "\n",
    "    # Text on the axis with the correct color\n",
    "    host.yaxis.label.set_color(p1.get_color())\n",
    "    par1.yaxis.label.set_color(p2.get_color())\n",
    "\n",
    "    # Make little spikes for different y-axis\n",
    "    tkw = dict(size=30, width=1.6)\n",
    "    host.tick_params(axis='y', colors=p1.get_color(), **tkw)\n",
    "    par1.tick_params(axis='y', colors=p2.get_color(), **tkw)\n",
    "    host.tick_params(axis='x', **tkw)\n",
    "\n",
    "    # Find out the potential control and the escalation days\n",
    "    potential_control_point = df_countries.loc[(df_countries['country'] == current_country), 'control_point'].tolist()[0]\n",
    "    potential_escalation_point_deaths = df_countries.loc[(df_countries['country'] == current_country), 'escalation_point_deaths'].tolist()[0]\n",
    "    potential_deaths_escalated_rapidly = df_countries.loc[(df_countries['country'] == current_country), 'deaths_escalated_rapidly'].tolist()[0]\n",
    "    potential_escalation_point_infections = df_countries.loc[(df_countries['country'] == current_country), 'escalation_point_infections'].tolist()[0]\n",
    "    \n",
    "    # Plot them, if they just exist\n",
    "    if str(potential_control_point) != 'NaT':\n",
    "        plt.axvline(potential_control_point, color='green')\n",
    "        print('The country got the epidemic under control (green line).')\n",
    "        \n",
    "    if str(potential_escalation_point_deaths) != 'NaT' and potential_deaths_escalated_rapidly == 'no':\n",
    "        plt.axvline(potential_escalation_point_deaths, color='black')\n",
    "        print('Later, the epidemic escalated but slowly which is the reason the country is in Group 3 (black line).')\n",
    "        \n",
    "    if str(potential_escalation_point_deaths) != 'NaT' and potential_deaths_escalated_rapidly == 'yes':\n",
    "        plt.axvline(potential_escalation_point_deaths, color='black')\n",
    "        print('Later the epidemic escalated rapidly in the country (black line).')\n",
    "        \n",
    "    if str(potential_escalation_point_infections) != 'NaT':\n",
    "        plt.axvline(potential_escalation_point_infections, color='red')\n",
    "        print('There has been manually found out when the epidemic escalated based on the infected data (red line).')\n",
    "        \n",
    "        \n",
    "        \n",
    "    #print('\\033[0m' + description[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Based on countries which did well against the epidemic during summer, understanding the impact of different traffic components to the spread of the epidemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Definition\n",
    "\n",
    "- Remaining traffic components: basic notation and T_t,r^R ja T hat\n",
    "\n",
    "\n",
    "\n",
    "### 4.2. Moral\n",
    "\n",
    "- The goal of this section is to find out which traffic components (retail, grocery, park, transit stations, workplace) were necessary to have reduced so that the epidemic was under control in a country.\n",
    "\n",
    "\n",
    "- The analysis of this section is done with Google's regional data, i.e. regions inside European countries\n",
    "\n",
    "\n",
    "- Traffic components of Group 1 -countries' regions are plotted in this section\n",
    "    - If it seems that a lot of these regions managed to increase the traffic of some component back to the baseline level, it indicates that this traffic component did not play very significant role in the spread of the epidemic.\n",
    "    \n",
    "\n",
    "- EXPLAINED WITH T-hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Get an overview on traffic component data of different regions where the epidemic did not escalate\n",
    "\n",
    "\n",
    "- These regions are in many ways heterogeneous but it may give an overview of the impact of different traffic components to the epidemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Conclusions\n",
    "\n",
    "- Many regions of group 1 -countries had their grocery traffic data close to the baseline level and park data much higher than the baseline level on average. This observation indicates these traffic components did not play a significant role in the pandemic.\n",
    "\n",
    "\n",
    "- Difficult to say anything reasonable about other traffic components based on the previous plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Based on countries where the epidemic escalated during the summer, understanding the impact of different traffic components to the spread of the epidemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Moral\n",
    "\n",
    "\n",
    "- This section is similar to the section 5\n",
    "    - The aim is to understand which traffic components had an impact to the spread of the epidemic\n",
    "    - The analysis is also made using Google's regional data\n",
    "\n",
    "\n",
    "- However, now regional analysis is done with regions which are in group 2 -countries. Indeed, in some regions the epidemic escalated and in some not! To find out this information, it is essential to get regional infected data.\n",
    "    - Getting regional infected data requires extra work because it is not as easily available as countrywise data\n",
    "    - Practically regional infected data is used over regional death data because there is no regional death data available for many countries\n",
    "\n",
    "\n",
    "- Let's assume out of the traffic data components can be found out the reason why the epidemic escalated in the region. There are two hypothesis for this statement.\n",
    "    1. Hypothesis: In the regions where the epidemic started to escalate, the level of some traffic component before the escalation was higher than in the regions where there was no escalation.\n",
    "        - Practically this hypothesis explains the escalation of the epidemic badly.\n",
    "    2. Hypothesis: In the regions where the epidemic started to escalate, some traffic component increased before the escalation.\n",
    "        - Practically this approach leads to much better findings.\n",
    "        \n",
    "        \n",
    "- AGAIN, Show with the new notations what next!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Get an overview of Spanish regions' traffic and infected data\n",
    "\n",
    "- Later other regions will be added. Now Spain is the only example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Plot the traffic level before the escalation and the infected coefficients of each region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis \n",
    "\n",
    "- In the regions where the epidemic started to escalate, the level of some traffic component was higher than in the regions where there was no escalation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "\n",
    "- Indeed, this hypothesis does not say too much at least about Spanish regions. This observation indicates epidemiological regional differences are big also inside a country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Plot the increase of infected and the change of traffic components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis \n",
    "\n",
    "-  In the regions where the epidemic started to escalate, some traffic component increased before the escalation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "\n",
    "- There were differences between the increments of traffic components\n",
    "    - Retail and public transport traffic data always increased before the epidemic escalated\n",
    "    - In some cases before escalation supermarket and workplace data increased and in some cases not\n",
    "\n",
    "\n",
    "- Indeed, retail and public transport traffic data seem to be a trigger for the escalation of the epidemic\n",
    "\n",
    "\n",
    "- However, it is difficult to say anything about the supermarket and workplace data based on this subsection because many there may be many traffic components which trigger the escalation of the epidemic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Conclusions of the section 5 and 6\n",
    "\n",
    "\n",
    "- Combining how big impact different traffic components had on the epidemic\n",
    "    - Retail: most likely had an effect\n",
    "        \n",
    "    - Supermarket: most likely did not have too much effect\n",
    "\n",
    "    - Park: did not have an effect\n",
    "         \n",
    "    - Transit station: most likely had an effect\n",
    "         \n",
    "    - Workplace: may have had an effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Getting an overview which countries were the most succesful against the epidemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Plot different countries' death and traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Conclusions\n",
    "\n",
    "\n",
    "- Different European countries are very difficult to compare with another\n",
    "\n",
    "\n",
    "- Different countriues should adapt very diffent strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. The findings of this research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. An analysis of potential mistakes\n",
    "\n",
    "- Different traffic components correlate with another which adds noise\n",
    "    - Example: A person uses public transport, goes to the supermarket, goes to a park and travels back home\n",
    "    \n",
    "    \n",
    "- Clustering traffic data in components always adds noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Discussion \n",
    "\n",
    "- The results which traffic components have the biggest impact on the spread of COVID-19 are intuitive: \n",
    "    - In places like supermarkets and pharmacies people can hold distance with other people relatively well.\n",
    "    - On contrary, in public transports people easily go very close to other people.\n",
    "\n",
    "\n",
    "- If the grocery traffic is okay but retail problematic, it indicates there are only some high-risk places related to retail where people should not go\n",
    "\n",
    "\n",
    "- Better research would be possible to do with more fine-grained Google's traffic data. \n",
    "    - Of course some clustering needs to be done\n",
    "\n",
    "\n",
    "- Coincidence plays a huge role what comes to the spread of the epidemic.\n",
    "\n",
    "\n",
    "- During summer, people act differently than during autumn\n",
    "\n",
    "\n",
    "- Discussion about masks related to this research\n",
    "\n",
    "\n",
    "- Analysis: Based on the observation of different traffic data components, these NPIs seem  to be important\n",
    "\n",
    "\n",
    "\n",
    "- This research indicates suppression-strategy can work for the whole epidemic before vaccine is developed in European countries. A new normal does not have to be anything to the massive lockdowns what was seen in Europe.\n",
    "\n",
    "\n",
    "- Section 8 is a reason why testing is important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Good articles\n",
    "\n",
    "- https://www.medrxiv.org/content/10.1101/2020.05.28.20116129v3.full.pdf\n",
    "    - NPI comparisons made between countries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
